%        File: sheet.tex
%     Created: Mon Mar 14 04:00 PM 2016 C
%
\documentclass[a4paper,twocolumn]{article}

\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}

\geometry{a4paper, margin={.3in, .3in}}

\title{Computer Vision 1 - Cheat Sheet}
\author{Andrea Jemmett}

\begin{document}
\maketitle

\section{Color Models}
Color is part of the electromagnetic spectrum with energy in the range from 380
to 780 nm wavelength. Most of the colors we perceive are a mixture of
wavelengths where the amount of energy at each wavelength is given by the
spectral energy distribution (SED). When white light shines upon an object, some
wavelengths are absorbed and other are reflected (a green object will reflect
light with wavelength around 500nm, other wavelengths will be absorbed). The
\textbf{hue} corresponds with the dominant wavelength of the SED;
\textbf{saturation} is defined as the proportion of pure light with respect to
white light needed to produce the color; \textbf{lightness} is the intensity of
the reflected light meanwhile \textbf{brightness} is the intensity of the light
source. Let $EH$ be the dominant wavelength in the SED and $EW$ the wavelength
contributing to the white light, then the \textit{hue} is equal to $EH$, the
\textit{saturation} equals to the difference $EH - EW$ and the
\textit{lightness} equals to the area underlined by the SED.

\begin{figure}[htpb]
	\centering
	\includegraphics[height=1.5in]{imgs/color-sed.png}
\end{figure}

Experiments have been conducted in which a human observer was asked to adjust
three knobs which control the intensity of a three primary colors so to match
the (perceived) color of the test light. The three primary lights were
additively mixed in and the knobs' values were recorded yielding the so called
color matching functions $\bar{r}(\lambda)$, $\bar{g}(\lambda)$ and
$\bar{b}(\lambda)$. The problem with these was that a negative amount of
at least one of the primaries was necessary to produce the full spectra. So the
CIE proposed a mathematical transformation, the \textbf{XYZ} model, which uses
another set of color matching functions: $\bar{x}(\lambda)$, $\bar{y}(\lambda)$
and $\bar{z}(\lambda)$.

The observer perceives color in terms of three color signals based on the
trichromacy theory and can be modeled as:
\begin{equation} \label{eq:color-perception}
C = \int_{\lambda} E(\lambda) S(\lambda) f_C(\lambda) d\lambda
\end{equation}
where $C \in \{R, G, B\}$, $E$ is the SPD, $S$ is the light reflected by objects
and $f_C$ is the color matching function. If we use $\bar{x}$, $\bar{y}$ and
$\bar{z}$ then we have the XYZ color space. To better represent graphically this
color space we can compute the $xyz$ values as follows:
\begin{equation}
x = \frac{X}{X + Y + Z} \quad y = \frac{Y}{X + Y + Z} \quad z = \frac{Z}{X + Y + Z}
\end{equation}
where we factor out the intensity. Since the chromaticity values sum to unity,
two elements are sufficient to represent a color. When the $x$ and $y$ values
are represented on a plane the chromaticity diagram is obtained. We can also
infer the hue from the chromaticity diagram: first we need to select a reference
white light, the hue is then the wavelength at the spectral curve that
intersects the line from reference light through the color point to the spectral
curve (this point is $G_2$). If $||G_1||$ is the distance from the color to the
white light and $||G_2||$ is the distance from $G_2$ to the white source, then
the saturation is given by $\frac{||G_1||}{||G_2||}$.

RGB values can be obtained using equation \ref{eq:color-perception} and the
$\bar{r}$, $\bar{g}$, and $\bar{b}$ color matching functions. The projection of
RGB points on the rgb chromaticity triangle is defined by:
\begin{equation}
r = \frac{R}{R + G + B} \quad g = \frac{G}{R + G + B} \quad b = \frac{B}{R + G + B}
\end{equation}
In the HSI chromaticity diagram, we compute hue and saturation in the following
way: by assuming white light we define a reference point of $r = g = b = 1/3$,
the saturation can than be computed as:
\begin{equation}
S_{rgb}(r, g, b) = \sqrt{(r - 1/3)^2 + (g - 1/3)^2 + (b - 1/3)^2}
\end{equation}
or
\begin{equation}
S(R, G, B) = 1 - \frac{min(R, G, B)}{R + G + B}
\end{equation}
while the hue is given by:
\begin{equation}
H_{rgb}(r, g, b) = arctan(\frac{r - 1/3}{g - 1/3})
\end{equation}
or
\begin{equation}
H(R, G, B) = arctan(\frac{\sqrt{3}(G - B)}{(R - G) + (R - B)})
\end{equation}

A color invariant system contains color invariant models that are more or less
insensitive to the varying image conditions. For matte surfaces
\textit{RGB} is sensitive to orientation while \textit{rgb} is (assuming
constant white light) insensitive to orientation, illumination direction and
intensity (similarly are S and H). For shiny surfaces H is color invariant.


\section{Surface Reflection}
The Bidirectional Reflectance Distribution Function (BRDF) is the most general
model of light scattering. Describes how much light arriving at an incident
direction $\mathbf{v_i}$ is emitted in a reflected direction $\mathbf{v_r}$. It
can be written as a function of the angles of incident and reflected light like so:
\begin{equation}
f_{BRDF}(\theta_i, \phi_i, \theta_r, \phi_r) =
	\frac{radiance\_of(\theta_r, \phi_r)}{irradiance\_at(\theta_i, \phi_i)}
\end{equation}
Typically BRDF can be split into \textit{diffuse} and \textit{specular}
components. The \textbf{diffuse} component (\textit{Lambertian} or \textit{matte}
reflection) scatters light uniformly in all directions and is associated with
the phenomena of \textit{shading}. Light is scattered uniformly across all
directions (the BRDF is constant):
\begin{equation}
f_d(\theta_i, \phi_i, \theta_r, \phi_r) = \frac{\rho_d}{\pi}
\end{equation}
but the amount of light depends on the angle between the incident direction and
the surface normal $\theta_i$ (is independent of the viewing direction
$\mathbf{v}$):
\begin{equation}
radiance\_of_d = \frac{\rho_d}{\pi} I cos(\theta_i)
\end{equation}
where $\rho_d$ is the surface albedo and $I$ is the source intensity.
This is because the surface area exposed to a given amount of light becomes
larger at oblique angles. The \textbf{specular} (glossy or highlight) BRDF reflection
depends strongly on the outgoing light direction. All the incident light energy
is reflected in a \textit{single} direction (only when $v_i = v_r$). So the
mirror BRDF is a delta function:
\begin{equation}
f_s(\theta_i, \phi_i, \theta_v, \phi_v) = \rho \delta(\theta_i - \theta_v)
\delta(\phi_i + \pi - \phi_v)
\end{equation}
\begin{equation}
radiance\_of_s = I f_d(\theta_i, \phi_i, \theta_v, \phi_v)
\end{equation}

Another reflectance model is the \textbf{Phong} model which uses an
\textit{ambient illumination} component besides diffuse and specular.


\section{Image Processing}
We can think of an image as a function from $\mathbb{R}^2$ to $\mathbb{R}$.

\subsection{Point Operators}
Act on a single point, that is locality in the image is lost.
\paragraph{Contrast and Brightness} also called \textit{gain} and
\textit{bias} are defined by the following operation:
\begin{equation}
g(\mathbf{x}) = a f(\mathbf{x}) + b
\end{equation}
where $a$ and $b$ are gain and bias respectively.
\paragraph{Gamma Correction} is used to remove the non-linear mapping between
the input radiance and the pixel values (invert gamma mapping):
\begin{equation}
g(\mathbf{x}) = [f(\mathbf{x})]^{\frac{1}{\gamma}}
\end{equation}
where $\gamma = 2.2$ is a reasonable fit for most cameras.
\paragraph{Histogram Equalization} is used to transform an image so that the
resulting histogram is flat. A pro of this is that we can show the entire color
range, but a flat histogram results in a muddy-looking picture. A solution to
this is to perform a \textit{partial} histogram equalization, that is blend the
full histogram equalization and an identity transform.

\subsection{Neighborhood Processing (Filtering)}
Act on a neighborhood of pixels, that is spatial information is preserved.
\subsubsection{Linear Filters}
This class of filters have the property of being linear (the response to the sum
of two signals is equal to the sum of the responses of the two signals
separately). It could be formulated as follows:
\begin{equation}
g(i, j) = \sum_{k,l} h(k, l) f(i + k, j + l)
\end{equation}
and is called a \textit{correlation}. A common variant is the following:
\begin{equation}
g(i, j) = \sum_{k,l} h(k, l) f(i - k , j - l)
\end{equation}
and is called a \textit{convolution}. Note that a filter correlated with an
image returns the flipped image, whereas the result of a convolution has the
same orientation because the sign of the offsets have been reversed.

Linear filters suffer from boundary effects, this is because the original image
is being padded with zero valued pixels wherever the convolution kernel extends
beyond the image boundaries. There exist a number of padding techniques:
\textit{zero} sets all pixels outside the image with zeros, \textit{constant}
sets all pixels to a specified border color, \textit{clamp} repeats edge pixels
indefinitely, \textit{cyclic} wraps pixels around the image and \textit{mirror}
reflected pixel along the edge.

\paragraph{Separable filtering}
The cost of performing a convolution is of $K^2$ operations per pixel. This
operation can be sped up by performing first a 1D horizontal and then vertical
convolution. This results in $2K$ operations per pixel. A kernel for which this
is possible is said to be \textit{separable}. So a 2D kernel $\mathbf{K}$ is
equal to the outer product of two 1D filters
$\mathbf{K}=\mathbf{v}\mathbf{h}^T$ in vertical and horizontal directions.
Determining if a kernel is separable is often done by inspection, using the
analytical form or using SVD.

\subsubsection{Common linear filters}
\paragraph{Box or moving average} simply takes the average in a $K^2$ window so has
	a smoothing effect; it's equal to convolving with a kernel of all ones
	and then scaling; for example:
	\begin{equation}
		\frac{1}{9} \quad \begin{bmatrix}
			1 & 1 & 1 \\
			1 & 1 & 1 \\
			1 & 1 & 1
		\end{bmatrix}
	\end{equation}
\paragraph{Sharpening} accentuate difference with local average:
	\begin{equation}
		\begin{bmatrix}
			0 & 0 & 0 \\
			0 & 2 & 0 \\
			0 & 0 & 0
		\end{bmatrix}
		\quad - \quad \frac{1}{9} \quad \begin{bmatrix}
			1 & 1 & 1 \\
			1 & 1 & 1 \\
			1 & 1 & 1
		\end{bmatrix}
	\end{equation}
\paragraph{Sobel} used to find edges in one direction:
	\begin{equation}
		\begin{bmatrix}
			-1 & -2 & -1 \\
			 0 &  0 &  0 \\
			 1 &  2 &  1
		\end{bmatrix}
	\end{equation}
	finds horizontal edges;
\paragraph{Gaussian} weights contribution of pixels by their nearness, so has a
	smoothing effect;
	\begin{equation}
		G_{\sigma} = \frac{1}{2\pi\sigma^2}e^{-\frac{x^2+y^2}{2\sigma^2}}
	\end{equation}
	the parameter $\sigma$ is a smoothing parameter, larger $\sigma$ corresponds
	to a blurrier image; removes high-frequency components and so acts as a
	low-pass filter; it is a \textit{separable} kernel; used to denoise Gaussian
	additive noise (not salt and pepper);
\paragraph{Laplacian of Gaussian} is a \textit{steerable} filter defined as:
	\begin{equation} \label{eq:log}
		\nabla^2 G = \frac{\partial^2 G}{\partial x^2} + \frac{\partial^2 G}{\partial y^2}
		= \left(\frac{x^2 + y^2}{\sigma^4} - \frac{2}{\sigma^2}\right) G
	\end{equation}

\subsubsection{Non-linear Filters}
In many case better performance can be achieved using a \textit{non-linear}
combination of pixels instead of a weighted average, as in linear filters. For
example in case of shot noise a Gaussian filter would result in the noisy pixels
being softened instead of removed.
\paragraph{Median filter} uses the median value from the pixel's neighborhood;
it's effective against shot noise because noisy pixel value usually lie away from
the true value in the neighborhood;
\paragraph{Bilateral filtering} the output pixel value depends on a weighted
combination of neighbour pixel values; weights are determined using both spatial
distance and intensity difference.


\section{Feature Extraction}
There are two kind of features. The first are \textbf{keypoint features} (interest points,
corners) and are single points on the image and are
characterized by their neighborhood using a \textit{descriptor}; these interest
points can then be matched in two different images to successively align them
for example. The feature matching pipeline is as follows:
\begin{enumerate}
  \item find a set of distinctive keypoints;
  \item define a region around each point;
  \item extract and normalize the region content;
  \item compute a local descriptor for each region;
  \item match local descriptors.
\end{enumerate}
It is important for keypoint features to be \textit{repeatable} and
\textit{distinctive}; also they need to be \textit{invariant} to translation,
rotation, scaling and other imaging parameters.
The second kind of features are \textbf{edges} that are used to get
orientation and local appearence of an object.

\subsection{Edge Detectors}
Edges are caused by a variety of factors, mostly discontinuities. For example
surface normal (change in shape), depth (objects at different perspective
levels), surface color (some lettering on a label) or illumination (shadows).
Analytically an edge is a rapid change in the image function. To find these
rapid changes we can apply a derivative to a signal so that the extrema of the
resulting signal are the rapid changes of the original input signal. If we apply
the derivation directly on the image we will obtain a noisier image back. To
cope with this we need to smooth the image first by applying a Gaussian filter.
Because both convolution and differentiation are linear operators they commute
so that we can write
\begin{equation}
	\mathbf{J_{\sigma}} = \nabla (G_{\sigma} * I) = \nabla (G_{\sigma}) * I = G_{\sigma} * \nabla(I)
\end{equation}
and this simplifies the edge detection operator because we could reuse a
derivative of Gaussian for multiple images without ever computing the actual
image derivative. This is often called a \textit{Derivative of Gaussian} filter.
The $\sigma$ in this case determines the scale at which the filter is going to
find edges, but may also blur the edges too much.

\subsubsection{Canny Edge Detector}
The Canny operator transforms an input image into a binary edge map of thin
(having width of one pixel) edge segments. The first step is to compute the
\textit{image gradient} in the $x$ and $y$ directions. We can use the Sobel
operator, that provides an approximation of the partial derivatives, or the DoG.
Then from the image gradients $I_x$ and $I_y$ we can compute the
\textit{orientation} as $\theta = atan2(I_x, I_y)$ and round it to multiplies of
$\pi/4$. A step of \textit{non-maxima suppression} is employed to test whether a
gradient value $g(p)$ is maximal in the direction given by $\theta(p)$.
The final step is edge linking (or \textit{hysteresis}). By using two thresholds
$T_{low}$ and $T_{high}$ we link edgels found where $g(p) \ge T_{high}$ with
edgels $q$ found in its neighborhood where $g(q) \ge T_{low}$. Another parameter
that controls the results of the operator is the $\sigma$ in case of DoG; small
$\sigma$ means fine features, large $\sigma$ detects large scale edges.

\subsection{Blob Detectors}
Blob detection methods aim at detecting regions in an image that differ in
properties (brightness, color) comparing to surrounding regions; all points in a
blob can be considered in some sense similar to each other.

\subsubsection{Laplacian of Gaussian}
The LoG is defined by the Laplacian operator applied to a Gaussian kernel
(\ref{eq:log}) and is a circularly symmetric kernel suitable for blob detection.
With this operator edges are identified by its zero-crossings.
\begin{figure}[htpb]
				\centering
				\includegraphics[height=1.5in]{imgs/gauss-profile.png}
\end{figure}
It is possible to build a \textit{pyramid} of Laplacians in the \textit{scale
space} by applying the LoG operator repeatedly with different scales (controlled
by $\sigma$) so to obtain different layers of Laplacian responses that may be
used to identify blobs at different scales. We define the \textit{characteristic
scale} as the scale that produces the highest peak of Laplacian response.

\subsection{Color Edge Detection and Classification}
While most edge detectors have been developed for grayscale images, color images
can provide useful information. For example edges between iso-illuminants
(colors that have the same illuminance) fail to be detected by grayscale edge
operators. A simple approach is to combine outputs of grayscale detectors
applied to each color band separately. A better approach is to compute the
\textit{oriented energy} in each channel (by using a steerable filter like DoG
or LoG), compute the gradient by summing gradient magnitudes separately:
\begin{equation}
	|\nabla C(x, y)| = \sqrt{R_x^2 + R_y^2} + \sqrt{G_x^2 + G_y^2} + \sqrt{B_x^2 + B_y^2}
\end{equation}
or using the Euclidean metric:
\begin{equation}
	|\nabla C(x,y)| = \sqrt{R_x^2 + R_y^2 + G_x^2 + G_y^2 + B_x^2 + B_y^2}
\end{equation}
or using eigenvalues.

We can use different gradients with different color spaces
to \textbf{classify} edges as being \textit{color edges}, \textit{shadows} or
\textit{highlights}. We need first to compute the color gradient in two color
spaces, one invariant to matte surfaces and one to shiny surfaces, such as
\textit{rgb} and \textit{Hue}, and define two thresholds $t_{rgb}$ and $t_H$.
Then if $|\nabla C_{rgb}| \ge t_{rgb} \wedge |\nabla C_H| < t_H$ we classify as
highlight edge, else if $|\nabla C_H| \ge t_H$ classify as color edge else
classify as shadow edge. An application of this technique is \textit{shadow removal}.
First we classify shadow edges in each color band, then the image is separated
into invariant and shadow image and the latter is subtracted (in log space) from
the original image.

\subsection{Harris Corner Detector}
We can distinguish different kind of structures for features, based on their
dimensionality; at 0D we have single points that are not useful for matching; at
1D we found edges that can be localized in 1D, that is we can move along the
edge; at 2D we can localize \textit{corners}, that is we can identify a feature
by moving along two directions, finding a corner. This issue is commonly called
\textit{aperture problem}. These ideas can be formalized using the simplest
possible matching criterion between two image patches, the sum of square
differences (SSD):
\begin{equation} \label{eq:ssd}
	E_{SSD}(u,v) = \sum_i w(x_i,y_i) [I_1(x+u,y+v) - I_0(x,y)]^2
\end{equation}
where $I_0$ and $I_1$ are two images being compared, $w$ is a window,
$\mathbf{u} = [u, v]$ and the summation is over all pixels in the patch (note
that this is the same formulation for Optical Flow estimation).

Because we don't know which other image locations the feature will end up being
matched against, we can only compute the so called \textit{auto-correlation}
comparing an image patch with itself with respect to small variations $\Delta\mathbf{u}$:
\begin{equation}
	E_{AC}(\Delta u, \Delta v) = \sum_i w(x_i,y_i) [I(x+\Delta u, y+\Delta v) - I(x,y)]^2
\end{equation}
Using a Taylor Series expansion of the image function $I(\mathbf{x_i}+\Delta
\mathbf{u} \equiv I(\mathbf{x_i}) \nabla I(\mathbf{x_i})\Delta \mathbf{u}$ we
can approximate the auto-correlation surface as:
\begin{equation}
	E_{AC}(\Delta \mathbf{u})=\Delta \mathbf{u}^T \mathbf{A} \Delta \mathbf{u}
\end{equation}
where
\begin{equation}
	\mathbf{A} = w * \begin{bmatrix}
		I_x^2 & I_x I_y \\
		I_x I_y & I_y^2
	\end{bmatrix}
\end{equation}
and $I_x$ and $I_y$ represents gradients in $x$ and $y$ directions. If we
consider a single pixel $\mathbf{p}$, we can define
\begin{equation}
	\mathbf{G_p} = \begin{bmatrix}
		I_x^2(\mathbf{p}) & I_x(\mathbf{p}) I_y(\mathbf{p}) \\
		I_x(\mathbf{p}) I_y(\mathbf{p}) & I_y^2(\mathbf{p})
	\end{bmatrix}
\end{equation}
This pixel is a corner feature if the eigenvalues $\lambda_1$ and $\lambda_2$
are both large and of similar magnitude. We can define the \textit{cornerness
measure} as
\begin{equation}
	H(\mathbf{p}) = det(\mathbf{G_p}) - \alpha Tr(\mathbf{G_p})
\end{equation}
with $0.04 \le \alpha \le 0.06$. Due to the properties of eigenvalues we can
write
\begin{equation}
	H(\mathbf{p}) = \lambda_1 \lambda_2 - \alpha (\lambda_1 + \lambda_2)
\end{equation}
The response function $H$ depends on the eigenvalues of $A$ so that if it is
large there is a corner, is negative with large magnitude for an edge and
$|H|$ is small for a flat region.

\paragraph{Harris Corner Detector algorithm}
\begin{enumerate}
	\item Compute $x$ and $y$ derivatives of image
		$$I_x=G_{\sigma}^x * I \qquad I_y = G_{\sigma}^y * I$$
	\item Compute product of derivatives pixel-wise to build matrix $\mathbf{A}$
	\item Convolve each of these images with a larger Gaussian
	\item Build a matrix $\mathbf{G}$ for each pixel
	\item Compute the cornerness measure $H$ for each pixel
	\item Threshold on values of $H$ and compute non-maxima suppression
\end{enumerate}


\subsection{Template Matching}
The goal is to find an image template inside a bigger image (scene). There are
two methods. The first one consists in using a SSD (eq. \ref{eq:ssd}) that signals how much the
template differ from the examined window around a pixel. Because of this we need
to take a $1 - \sqrt{SSD}$ to than identify the maxima as true detections.
A second approach consists in using the normalized cross-correlation as
\begin{equation} \label{eq:xcorr}
	h[m,n]=\frac{\sum_{k,l}(g[k,l]-\bar{g})(f[m-k,n-l]-\bar{f}_{m,n})}
		{\sqrt{\sum_{k,l}(g[k,l]\bar{g})^2\sum_{k,l}(f[m-k,n-l]-\bar{f}_{m,n})^2}}
\end{equation}
where $\bar{g}$ is the mean template and $\bar{f}_{m,n}$ is the mean image patch.
The normalized cross-correlation responds positively for similar patches, so we
can apply a thresholding operation directly on it to find the template.

SSD is usually faster but is sensitive to the overall intensities. Normalized
cross-correlation methods instead are slower but invariant to local average
intensity and contrast.



\section{Tracking}
The goal of tracking techniques is to find the same set of features in a
sequence of two or more images. Unlike feature matching, where we search for
pairs of matches in two or more images, in tracking we identify a set of
features in the first frame and we \textit{track} their movement across the
sequence. There are two main classes of tracking algorithms: in one we
\textit{search} for features (or templates) in a \textit{detects and track}
fashion, in the other learning methods are used to \textit{classify} patches and
train specific recognizers.

\subsection{Optical Flow}
The goal is to find for each pixel a velocity vector that says in which
direction it is moving and how quickly it is moving across the image.
This is the most general (and challenging) method for motion estimation.
The first step is to apply a Taylor expansion of the two images
\begin{multline} \label{eq:optic-flow-taylor}
	I(x+\delta x, y+\delta y, t+\delta t) = \\
		I(x,y,t)+\delta x\cdot\frac{\partial I}{\partial x}(x,y,t)+\delta
		y\cdot\frac{\partial I}{\partial y}(x,y,t)+\delta t\cdot\frac{\partial
		I}{\partial t}(x,y,t)
\end{multline}
as the value $\delta t$ we take the time difference between two subsequent
frames. With $\delta x$ and $\delta y$ we model the \textit{small} motion of one
pixel at time $t$ into another location at time $t+1$. Then we use the
\textit{brightness constancy assumption}
\begin{equation}
	I(x+\delta x,y+\delta y, t+\delta t)=I(x,y,t)
\end{equation}
to obtain that the right-hand side of equation \ref{eq:optic-flow-taylor} is
equal to zero. We than divide by $\delta t$ to obtain
\begin{equation}
	uI_x + vI_y + I_t = 0
	\label{eq:optic-flow-eq}
\end{equation}
where $u = u(x,y,t) = \delta x / \delta t$, $v = v(x,y,t) = \delta y / \delta t$,
$I_x = (\partial I / \partial x)(x,y,t)$, and similarly for $I_y$ and $I_t$.
Equation \ref{eq:optic-flow-eq} is known as the \textit{optic flow equation}.

Unfortunately this is a system of one equation in two unknowns. Lukas and Kanade
proposed a second constraint (the \textit{spatial motion constraint}) which
assumes motion constancy within pixel neighborhoods at time $t$ which means that
adjacent pixels in $I(x,y,t)$ have the same velocity vectors. To cope with noisy
data it is better to use a neighborhood of size $k > 2$ pixels thus having an
overdetermined system. It should also not be too large because we assume that
all pixels in the neighborhood have the same motion vector. This leads to the
following equation system
\begin{equation}
	\begin{bmatrix}
		I_{x1} & I_{y1} \\
		I_{x2} & I_{y2} \\
		\vdots & \vdots \\
		I_{xk} & I{yk}
	\end{bmatrix}
	\begin{bmatrix}
		u \\ v
	\end{bmatrix}
	= -\begin{bmatrix}
		I_{t1} \\ I_{t2} \\ \vdots \\ I_{tk}
	\end{bmatrix}
	\label{eq:optic-flow-lk}
\end{equation}
which can be written as
\begin{equation}
	\mathbf{A}\mathbf{u}=-\mathbf{b}
\end{equation}
This system can be solved in the least-square error sense yielding an optimal
solution for $\mathbf{u}=(u,v)$
\begin{equation}
	\mathbf{u}=(\mathbf{A^TA})^{-1}\mathbf{A^Tb}
\end{equation}
It can be solved given that $\mathbf{A^TA}$ is invertible, its two eigenvalues
should not be small (like for Harris corners) and should be well-conditioned.

\subsection{Template Tracking}
Tracking consists in searching for a target object, represented by a
\textit{template}, in a bigger image. The most simple form of template tracking
is modeled for translational displacement searching for the same template but
translated, meanwhile the majority of the tracking applications require more
sophisticated motion models like rotation, scaling or affine. For those models
the template is mapped into a candidate region of the image using a parametrized
transformation $\varphi(\mathbf{x;y})$ so that each pixel has its parameter
vector $\mathbf{y}$.

Because the high number of parameter we can limit the search space to a region
in the image around the previous object position so to exploit the general
assumption that the object won't have moved far. Then we align the template with
every possible candidate region and find the most similar accordingly to a
similarity measure. For this we can use the sum of square differences (eq.
\ref{eq:ssd}) or the (unnormalized) cross-correlation (eq. \ref{eq:xcorr}). The
first one we have to minimize, while the other is to be maximized. From this
operations we can have an estimate of the $\mathbf{y}$ parameters of motion.
Doing this with a brute-force approach might be time consuming and only feasible
for translational motion. To cope with this we can use Gradient Descent over the
SSD to obtain the update rule
\begin{equation}
	\mathbf{y_{n+1}}=\mathbf{y_n}-(\nabla^2SSD)^{-1}\nabla SSD
\end{equation}


\subsection{Mean Shift Tracking}
Mean shift is a non-parametric feature space analysis technique for locating the
maxima of a density function. The assumption is that features are sampled from a
\textit{probability distribution function}. The approach taken by Mean Shift is
to perform \textit{gradient ascent} to find the path towards a maxima in the underlying
PDF.

The first step consists in using a \textit{Kernel Density Estimator} to obtain
an estimation of the PDF of the form
\begin{equation}
	\tilde{f}(\mathbf{x})=\frac{1}{nh}\sum_{i=1}^{n}K\left(\frac{\mathbf{x-x_i}}{h}\right)
				\label{eq:kernel-density-est}
\end{equation}
where the kernel $K$ could be the Epanechnikov, uniform or normal. We can then
say that for a feature $\mathbf{z}$ we have a density function $q_\mathbf{z}$,
while the target candidate centered at location $\mathbf{y}$ has the feature
distributed accordingly to $p_\mathbf{z}(\mathbf{y})$. The problem is then to
find the discrete location $\mathbf{y}$ whose associated density function
$p_\mathbf{z}(\mathbf{y})$ is the most similar to the target density function
$q_\mathbf{z}$.

The measure used to compute this error is the \textit{Bhattacharyya
Coefficient}, defined as
\begin{equation}
	\rho(\mathbf{y})\equiv\rho[p(\mathbf{y})q]=
	\int\sqrt{p_{\mathbf{z}}(\mathbf{y})q_{\mathbf{z}}}d\mathbf{z}
	\label{eq:bhatta-coeff}
\end{equation}
which acts as a similarity function and for a discrete density function
(as is in sampled images) the integral becomes a summation.



\section{Segmentation}
The goal of image segmentation is to break up the image into meaningful or
perceptually similar regions. In statistics this problem is also known as
\textit{cluster analysis}. Early techniques tend to use region splitting or
merging, while more recent algorithms often optimize some global criterion like
intra-region consistency and inter-region boundary lengths. In clustering
methods there are two approaches: \textit{top-down} in which we divide regions
that likely belong together and \textit{bottom-up} in which we group regions
with similar features. When segmenting an image the result can be an
over-segmented (where the segmentation is too fine, resulting in segmenting
objects) or under-segmented (where the segmentation in too coarse, resulting in
missing to cluster objects correctly).

\subsection{K-means}
In K-means we iteratively reassign points to the nearest cluster center. The
algorithm is as follows:
\begin{enumerate}
	\item Initialize $K$ cluster centers at random
	\item Assign each point to the closest cluster center using a distance measure
	\item Update the cluster centers using the mean of points in that cluster
	\item Repeat steps 2-3 until not points are reassigned
\end{enumerate}
So the only design choices to be made are on the number of clusters $K$ and the
distance metric to use (traditionally Euclidean). It is guaranteed that it will
converge to a local minimum.

As features we could use different ones such as intensities, different color
models or something called a \textit{filter bank}. When using filter banks we
use a set of filters so that when we apply those $n$ filters we get back $n$
features that later we can use for segmentation. Another filter that we can use
to extract useful features for clustering is a \textit{Gabor filter}. This two
approaches are particularly useful for textured images.

\subsection{Split-and-Merge}
To split an image into regions a simple technique consists in computing the
image histogram and then find a threshold that that best separates the large
peaks in the histogram.

Merging techniques combine adjacent regions whose average color difference is
below a threshold or whose regions are too small. Other approaches use distance
measure to aggregate data points or regions.

\subsection{Mean Shift Segmentation}
We have already seen the Mean Shift algorithm for tracking. For segmentation the
intuition is the same, the features in this case are not templates or keypoints,
but we want to cluster all image pixels by estimating the feature space PDF and
assigning pixels that lead to nearby modes to the same cluster.

The algorithm works as follows:
\begin{enumerate}
	\item Choose a kernel function $K$ and a bandwidth $h$
	\item For each pixel:
		\begin{enumerate}
			\item Center a window at that pixel
			\item Compute the mean of the data in the search window
			\item Center the search window at the new mean location
			\item Repeat b-c until convergence
		\end{enumerate}
	\item Assign pixels that lead to nearby modes to the same cluster
\end{enumerate}
It produces good general-practice segmentation, is flexible in number and shape
of regions and is robust to outliers (K-means is not). As a con it is not
suitable for high-dimensional feature spaces.

\subsection{Normalized Cuts}
The normalized cuts technique tries to find \textit{affinities} between near
pixels and tries to separate groups that are connected by weak affinities.
Lets consider a \textit{graph} representation of an image where nodes are pixels
and each node is connected with any other pixel. Each link has a weight
$w_{pq}$ that represents the similarity of the connected nodes. The cut
between two groups $A$ and $B$ is
\begin{equation}
	cut(A,B) = \sum_{i \in A, j \in B} w_{ij}
\end{equation}
We then define the \textit{normalized cut} as a segmentation metric
\begin{equation}
	Ncut(A,B)=\frac{cut(A,B)}{assoc(A,V)}+\frac{cut(A,B)}{assoc(B,V)}
\end{equation}
where $assoc(A,A)=\sum_{i\in A,j\in A}w_{ij}$ is the association (sum of all
weights) within a cluster and $assoc(A,V)=assoc(A,A)+assoc(A,B)$ is the sum of
all weights associated with nodes in $A$. We could also consider
$assoc(A,V)=volume(A)$ where it intends the sum of costs of all edges that touch $A$.


\section{Feature-based Alignment}
Feature-based alignment consists in estimating the motion between two or more
sets of 2D or 3D points. We need to find transformation parameters that link
the motion of a set of points matched in two frames.

\subsection{RANSAC}
RANdom SAmple Consensus is an iterative technique to estimate those transformation
parameters. Input to the algorithm is a set of data which follow the model,
called \textit{inliers} and a set of points that do not follow the model, called
\textit{outliers}. For example the set of inliers and outliers may be a noisy
representation of a line $y=ax+b$ where the task is to estimate $a$ and
$b$, but the model could also be rotation, scaling or affine.

For feature matching we are given two images with keypoints and descriptors for
each one. The RANSAC loop then goes as follows:
\begin{enumerate}
	\item Select $s$ feature pairs at random
	\item Estimate transformation parameters using $s$ pairs
	\item Compute the transformation for each point using the parameters
	\item Count the number of inliers $p$ where $SSD(p',p_t)<\varepsilon$
		($p'$ is the supposed match and $p_t$ is $p$ transformed using params)
	\item Keep the parameters that lead to the largest set of inliers
	\item Re-estimate transformation using inliers only
\end{enumerate}
The required minimum number of iterations is given by the following relation
\begin{equation}
	N \ge \frac{\log(1-P)}{\log(1-p^s)}
\end{equation}
where $P$ is the total probability of success after $N$ iterations and
$p$ is the probability that any given correspondence is valid (or the percentage
of outliers).


\section{Object Recognition}
There are different classes of \textit{recognition}: \textit{object detection}
is scanning an image to determine where a match may occur, \textit{instance
recognition} is trying to find a specific known object, \textit{category or
class} recognition is trying to associate a label to an image.

\subsection{Features for Recognition}
The first step in every recognition system is that of extracting image features.
To do so we compute a set of descriptors that characterize the keypoint or the
image patch. Most techniques consist in computing histograms of some image
property such as color texture or depth.

\paragraph{Scale Invariant Feature Transform} descriptors are formed by computing the
gradient at each pixel in a local window around a detected feature, using
the appropriate level of the Gaussian pyramid at which the keypoint was
detected. The first step is called \textit{orientation assignment} in which the
gradient magnitude and orientation are computed using pixel differences on the
Gaussian-smoothed image. An \textit{orientation histogram} with 36 bin is
created with each bin covering 10 degrees. This step ensures invariance to
rotation. Next step we compute the descriptor vectors which are highly
distinctive and invariant to illumination changes. Here a set of orientation
histograms is created for a $4 \times 4$ pixels neighborhood each with 8 bins.
These histograms are computed from magnitudes and orientation values of samples
in a $16 \times 16$ window around the keypoint such that each histogram contains
samples from a $4 \times 4$ subregion of the original neighborhood region. The
descriptor is then a vector of all values of these histograms. Since there are
$4 \times 4 = 16$ histograms each with 8 bins, the descriptor vector has 128
elements.

\subsection{Bag of Features}
This is a simple model for object detection (especially \textit{category
detection}) that takes inspiration from the \textit{bag of word} model used in
text classification and NLP. This algorithm simply computes the distribution
(histogram) of visual words found in a query image and compared this
distribution to those found in the training images.

Image features are represented using descriptors such as SIFT. A good descriptor
must be chosen carefully based on the application. Then a set of
\textit{codewords} (analogous to text words in a document) and a \textit{codebook}
(analogous to a word dictionary) are generated. A simple method to achieve this
is to perform K-means clustering over all the vectors; codewords are then
defined as the centers of the learned clusters. The number of clusters is the
codebook size. The final step is learn a classifier such as Naive Bayes, SVM or
Boosting that uses the frequencies of visual words of an image as features.

Instead of quantizing feature vectors into visual words there's a technique for
computing directly an approximate distance between two variably sized
collections of feature vectors. It consists in binning the feature vectors into
a multi resolution pyramid in feature space and counting the number of features
that land in corresponding bins.

\subsection{Face Detection}
Before face recognition can be applied to a general image, the locations and
sizes of any faces must be found. Of all the detectors currently in use, the
\textit{Viola/Jones} detector is probably the best known and widely used. Their
technique was the first to introduce \textit{boosting} (which involves training
a series of increasingly discriminating simple classifiers and then blending
their outputs) to the computer vision community.

In Viola and Jones' face detector the features are differences of rectangular
regions in the input patch. The computation of this filter can be done quickly
using \textit{integral images} which compute a value at each pixel that is the
sum of the pixel values above and to the left of it (inclusive). Let $A$,
$B$, $C$ and $D$ be the values of the integral image at the corners of a
rectangle. Then the sum of original image values within the rectangle can be
computed as $A - B - C + D$.

\subsection{Face Recognition}
Some of the earliest approaches in face recognition involved finding distinctive
image features, such as eyes, nose and mouth, and measuring the distance between
these feature locations. More recent approaches rely on comparing gray-level
images projected onto lower dimensional spaces called \textit{eigenfaces} and
jointly modeling shape and appearence variations.

\subsubsection{Eigenfaces}
Eigenfaces rely on the observation that an arbitrary face image
$\mathbf{x}$ can be compressed and reconstructed by starting with a mean image
$\mathbf{m}$ and adding a small number of scaled signed images $\mathbf{u}_i$
\begin{equation} \label{eq:eigenfaces-compression}
	\tilde{\mathbf{x}}=\mathbf{m}+\sum_{i=0}^{M-1}a_i\mathbf{u}_i
\end{equation}
where the signed basis images can be derived from an ensemble of training images
using \textit{principal component analysys} (PCA).

Starting from a collection of training images ${\mathbf{x}_j}$ we compute the
mean image $\mathbf{m}$ and the covariance matrix
\begin{equation}
	\mathbf{C}=\frac{1}{N}\sum_{j=0}^{N-1}\mathbf{(x_j-m)(x_j-m)^T}
\end{equation}
We can then apply an eigenvalue decomposition to represent this matrix as
\begin{equation}
	\mathbf{C}=\mathbf{U\Lambda U^T}=\sum_{i=0}^{N-1}\lambda_i\mathbf{u_iu_i^T}
\end{equation}
where the values $\lambda_i$ are the eigenvalues of the covariance matrix and
the $\mathbf{u}_i$ are the eigenvectors. For generic pictures those vectors are
called \textit{eigenpictures}, for faces those are the \textit{eigenfaces}.
Two important properties are that the optimal coefficients can be computed from
the eigenfaces
\begin{equation}
	a_i = (\mathbf{x - m}) \cdot \mathbf{u}_i
\end{equation}
and that the eigenvalues are sorted in decreasing order so truncating the
approximation given in (\ref{eq:eigenfaces-compression}) at any point $M$ gives
the best possible approximation between $\mathbf{x}$ and $\tilde{\mathbf{x}}$.
The distance of a projected image $\tilde{\mathbf{x}}$ to a mean image
$\mathbf{m}$ can be written as
\begin{equation}
	\text{DIFS}(\tilde{\mathbf{x}},\mathbf{m})=
		||\mathbf{\tilde{x}-m}||=\sqrt{\sum_{i=0}^{M-1}a_i^2}
\end{equation}
where DIFS stands for distance in \textit{face space}.




\newpage

\appendix
\section{Linear Algebra}
\paragraph{Trace} of a $n \times n$ matrix $\mathbf{A} = (a_{ij})$:
\begin{equation}
	Tr(\mathbf{A}) = \sum_{i=1}^n a_{ii}
\end{equation}
\paragraph{Determinant} of a $2 \times 2$ matrix $\mathbf{A} = (a_{ij})$ is:
\begin{equation}
	det(\mathbf{A}) = a_{11} a_{22} - a_{12} a_{21}
\end{equation}
for a $3 \times 3$ matrix is:
\begin{multline*}
	det(\mathbf{A}) = a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} +
	a_{13} a_{21} a_{32} + \\ - a_{13} a_{22} a_{31} - a_{12} a_{21}
	a_{33} - a_{11} a_{23} a_{32}
\end{multline*}
\paragraph{Eigenvalues} of an $n \times n$ matrix $\mathbf{A}$ are the $n$
solutions to the \textit{characteristic equation} $det(\mathbf{A} - \lambda
\mathbf{I}) = 0$. The determinant of a square matrix is equal to the product
of its eigenvalues, and the trace is equal to the sum of the eigenvalues.


\end{document}

