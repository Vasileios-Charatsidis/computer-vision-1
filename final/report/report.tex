\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{float}


\title{
	{Computer Vision 1 - Final Assignment\\
	 Bag-of-Words based Image Classification}
}
\author{
Selene Baez Santamaria (10985417) - Andrea Jemmett (11162929)}
\date{\today}

\begin{document}

\maketitle


\section{Feature Extraction and Description}
The first step in a Bag-of-Words image classification pipeline consists in
extracting features from a collection of images to successively use them to
build a visual vocabulary. To do this we extract SIFT descriptors from images
using \textit{VLFeat}. We defined a function in \texttt{sift\_descriptors.m}
that gets as input an image, the type of SIFT descriptors to extract (RGB, rgb
or opponent) and whether descriptors need to be extracted from keypoints or
densely and returns the extracted SIFT descriptors.

To extract SIFT features from color images we first convert the input image in
the requested color space. To do this we used a function that we created for
another assignment, \texttt{convert\_color\_space.m}. Then for keypoint
descriptors we find a set of keypoints on the intensity image and then we use
those frames to get local descriptors in each separate channel. This way if we
detect $n$ keypoints, the normal SIFT descriptors will have size 128, for color
SIFT we will have $128*3$ so that the final descriptors matrix has size $n
\times 128*3$.  For densely extracted features instead, we do not need to
extract keypoints on the intensity image and features are extracted using a grid
where the number of descriptors depends on the grid size.


\section{Building Visual Vocabulary}
Once we have a set of images represented as descriptors we use a clustering
technique to group together the most similar to each other until we reach a
stable set of $K$ clusters. We use \textit{Kmeans} through the use of the
function \texttt{vl\_kmeans} offered by VLFeat. The clusters resulting from the
operation become the set of words used in the vocabulary, so that $K$ controls
the size of the vocabulary.


\section{Features Quantization}
Once we have a vocabulary of visual words we can transform an image as a
collection of visual words. To do this we created two functions:
\texttt{as\_bag.m} and \texttt{hist\_from\_bags.m}.

The former gets as input an image, the vocabulary and SIFT parameters like the
type (RGBSIFT, rgbSIFT and opponentSIFT) and if we want dense- or keypoint-based
descriptors. For each feature vector extracted (of size 384) we find the visual
word in the vocabulary that is the closest to the feature vector. We compute
their difference and then the norm of the resulting, assigning the visual word
in the vocabulary that has the lowest norm value.

The latter is responsible of converting a collection of images represented as
bag of visual words into normalized histograms of visual words count; that is we
count how many times a visual word appears in a bag and then we normalize this
number in the range $[0, 1]$; we do this in order to improve successive
classification performance.


\section{Classification}



\section{Evaluation}


\end{document}

